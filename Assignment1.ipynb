{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTOwylHGTnTj"
      },
      "outputs": [],
      "source": [
        "!pip install wandb\n",
        "import wandb\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(x):\n",
        "  return np.maximum(0, x)\n",
        "\n",
        "def sigmoid(x):\n",
        "  clip_x = np.clip(x, -500, 500)  # Clipping x to avoid overflow\n",
        "  return 1 / (1 + np.exp(-clip_x))\n",
        "\n",
        "def _tanh(x):\n",
        "  clip_x = np.clip(x, -500, 500)  # Clipping x for uniformity\n",
        "  return np.tanh(clip_x)"
      ],
      "metadata": {
        "id": "8t045aEAUEOc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## writing the code in the classwise fashion\n",
        "class NeuralNetwork:\n",
        "  def __init__(self, inputSize, hiddenLayers, outputSize, sizeOfHiddenLayers, batchSize, learningRate, initialisationType, optimiser, epochs, activationFunc, weightDecay, isWandb = False, lossFunc = \"cross_entropy\", dataset = \"fashion_mnist\"):\n",
        "    # initialising model parameters\n",
        "    nodes_in_layers = []\n",
        "    for i in range(hiddenLayers):\n",
        "      nodes_in_layers.append(sizeOfHiddenLayers)\n",
        "    nodes_in_layers.append(outputSize)\n",
        "    if dataset == \"fashion_mnist\":\n",
        "      (X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()\n",
        "      X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)\n",
        "    elif dataset == \"mnist\":\n",
        "      (X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "      X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "    # normalsing and resisizing all the images\n",
        "    X_train = X_train/255.0\n",
        "    X_test  = X_test/255.0\n",
        "    X_val   = X_val/255.0\n",
        "\n",
        "    X_train = X_train.reshape(X_train.shape[0], 784).T\n",
        "    X_test = X_test.reshape(X_test.shape[0], 784).T\n",
        "    X_val = X_val.reshape(X_val.shape[0], 784).T\n",
        "\n",
        "    self.X_train = X_train\n",
        "    self.Y_train = Y_train\n",
        "    self.X_val   = X_val\n",
        "    self.Y_val   = Y_val\n",
        "    self.X_test  = X_test\n",
        "    self.Y_test  = Y_test\n",
        "\n",
        "    self.inputSize = inputSize\n",
        "    self.outputSize= outputSize\n",
        "    self.batchSize = batchSize\n",
        "    self.layers = hiddenLayers + 1\n",
        "    self.nodes  = nodes_in_layers\n",
        "    self.initialisationType = initialisationType\n",
        "    self.betha1 = 0.9\n",
        "    self.betha2 = 0.999\n",
        "    self.betha  = 0.9\n",
        "    self.epsilon= 1e-8\n",
        "    self.Weights= {}\n",
        "    self.Baises = {}\n",
        "    self.optimiser = optimiser\n",
        "    self.epochs = epochs\n",
        "    self.learningRate = learningRate\n",
        "    self.activationFunc = activationFunc\n",
        "    self.isWandb = isWandb\n",
        "    self.weightDecay = weightDecay\n",
        "    self.lossFunc = lossFunc # \"cross_entropy\" or \"MSE\"\n",
        "    self.dataset = dataset\n",
        "\n",
        "\n",
        "  def Initialise(self):\n",
        "    # initialising weights and biases as a key value pair\n",
        "    W = {}\n",
        "    B = {}\n",
        "\n",
        "    PreActivation = {}\n",
        "    Activation = {}\n",
        "\n",
        "    # adding input layer\n",
        "    LayerWise = self.nodes\n",
        "    LayerWise.insert(0, self.inputSize)\n",
        "\n",
        "    # initialisation of weights and baises\n",
        "    for i in range(self.layers):\n",
        "      if self.initialisationType == \"random\":\n",
        "        W[i+1] = 0.01*np.random.randn(LayerWise[i+1], LayerWise[i])\n",
        "        B[i+1] = 0.01*np.random.randn(LayerWise[i+1], 1)\n",
        "      if self.initialisationType == \"Xavier\":\n",
        "        W[i+1] = np.random.randn(LayerWise[i+1], LayerWise[i]) * np.sqrt(2. / (LayerWise[i] + LayerWise[i+1]))\n",
        "        B[i+1] = np.zeros((LayerWise[i+1], 1))\n",
        "\n",
        "      # preactivation and activation will have same size\n",
        "      PreActivation[i+1] = np.zeros((LayerWise[i+1], 1))\n",
        "      Activation[i+1]    = np.zeros((LayerWise[i+1], 1))\n",
        "\n",
        "    del LayerWise[0]\n",
        "\n",
        "    self.Weights = W\n",
        "    self.Baises  = B\n",
        "    self.PreActivation = PreActivation\n",
        "    self.Activation = Activation\n",
        "\n",
        "    return W,B,PreActivation,Activation\n",
        "\n",
        "  def InitialiseEmptyWeightsAndBiases(self):\n",
        "    W = {}\n",
        "    B = {}\n",
        "    LayerWise = self.nodes\n",
        "\n",
        "    LayerWise.insert(0, self.inputSize)\n",
        "    for i in range(self.layers):\n",
        "      W[i+1] = np.zeros((LayerWise[i+1], LayerWise[i]))\n",
        "      B[i+1] = np.zeros((LayerWise[i+1], 1))\n",
        "    del LayerWise[0]\n",
        "\n",
        "    return W,B\n",
        "\n",
        "  def FeedForward(self, x, W, B, preActivation, activation):\n",
        "    # no of layers\n",
        "    n = len(W)\n",
        "    y = x\n",
        "    for i in range(1, n+1):\n",
        "      preActivation[i] = np.dot(W[i], y) + B[i]\n",
        "      if self.activationFunc == \"sigmoid\":\n",
        "        activation[i] = sigmoid(preActivation[i])\n",
        "      elif self.activationFunc == \"tanh\":\n",
        "        activation[i] = _tanh(preActivation[i])\n",
        "      elif self.activationFunc == \"relu\":\n",
        "        activation[i] = relu(preActivation[i])\n",
        "\n",
        "      y = activation[i]\n",
        "\n",
        "    # last layer we don't need activation\n",
        "    y = preActivation[n]\n",
        "    # doing softmax doing the each column wise\n",
        "    exp_y = np.exp(y - np.max(y, axis=0, keepdims=True))  # Improve numerical stability\n",
        "    y = exp_y / np.sum(exp_y, axis=0, keepdims=True)\n",
        "    return y\n",
        "\n",
        "  def BackWardPropogation(self, X, y_corr, W, preActivation, activation, y_hat):\n",
        "    # y_hat is the prediction and y_corr is the correct class\n",
        "    dw = {}\n",
        "    db = {}\n",
        "\n",
        "    # these many points are there in the batch\n",
        "    batch_Size = y_corr.shape[0]\n",
        "    y = np.zeros([10, batch_Size])\n",
        "    # this y is encoded in 10*batchsize with each col being for one point that will be one\n",
        "\n",
        "    for ind in range(batch_Size):\n",
        "      y[y_corr[ind]][ind] = 1\n",
        "\n",
        "    if self.lossFunc == \"cross_entropy\":\n",
        "      da = y_hat - y\n",
        "    elif self.lossFunc == \"MSE\":\n",
        "      da = (y_hat - y)*(y_hat)*(1 - y_hat)\n",
        "      da = da / self.batchSize\n",
        "\n",
        "    activation[0] = X\n",
        "    layer = len(W)\n",
        "    dh = da #used for finding next layer\n",
        "    while layer >= 1:\n",
        "      dw[layer] = np.dot(da, activation[layer-1].T)\n",
        "      db[layer] = da.sum(axis=1, keepdims=True)\n",
        "      if layer > 1:\n",
        "        dh = np.dot(W[layer].T, da)\n",
        "        if self.activationFunc == \"sigmoid\":\n",
        "          dg = activation[layer-1] * (1 - activation[layer-1])\n",
        "        elif self.activationFunc == \"tanh\":\n",
        "          dg = (1 + activation[layer-1]) * (1 - activation[layer-1])\n",
        "        elif self.activationFunc == \"relu\":\n",
        "          dg = np.where(preActivation[layer-1] > 0, 1, 0)\n",
        "\n",
        "        # dg = activation[layer-1] * (1 - activation[layer-1])\n",
        "        da = dh * dg\n",
        "        # hedamant product\n",
        "      layer -= 1\n",
        "\n",
        "    # L2 regularisation\n",
        "    for i in range(1, self.layers+1):\n",
        "      dw[i] = dw[i] + self.weightDecay*W[i]\n",
        "\n",
        "    return dw, db\n",
        "\n",
        "  def FindAccuracyAndLoss(self, W, B, data, labels):\n",
        "    n = data.shape[1]\n",
        "    correct = 0\n",
        "    labels_one_hot = np.eye(10)[labels]\n",
        "    #running the data on the weights and baises\n",
        "    y = data\n",
        "    for i in range(1, self.layers+1):\n",
        "      preActivation = np.dot(W[i], y) + B[i]\n",
        "\n",
        "      if self.activationFunc == \"sigmoid\":\n",
        "        activation = sigmoid(preActivation)\n",
        "      elif self.activationFunc == \"tanh\":\n",
        "        activation = _tanh(preActivation)\n",
        "      elif self.activationFunc == \"relu\":\n",
        "        activation = relu(preActivation)\n",
        "      y = activation\n",
        "\n",
        "    # last layer we don't need activation\n",
        "    y = preActivation\n",
        "    # doing softmax doing the each column wise\n",
        "    exp_y = np.exp(y - np.max(y, axis=0, keepdims=True))  # Improve numerical stability\n",
        "    y = exp_y / np.sum(exp_y, axis=0, keepdims=True)\n",
        "    loss = 0\n",
        "\n",
        "    for i in range(1, 1+self.layers):\n",
        "      loss += self.weightDecay * np.linalg.norm(W[i])\n",
        "\n",
        "    for i in range(n):\n",
        "      y_pred = np.argmax(y[:,i])\n",
        "      if labels[i] == y_pred:\n",
        "        correct += 1\n",
        "\n",
        "      if self.lossFunc == \"cross_entropy\":\n",
        "        loss += -1*np.log(y[:,i][labels[i]] + 1e-9)\n",
        "      else:\n",
        "        loss += np.sum((y[:, i] - labels_one_hot[i]) ** 2)\n",
        "\n",
        "    return (correct*100/ n), (loss/n)\n",
        "\n",
        "  def predict(self, data):\n",
        "    n = data.shape[1]\n",
        "    y = data\n",
        "\n",
        "    for i in range(1, self.layers+1):\n",
        "      preActivation = np.dot(self.Weights[i], y) + self.Baises[i]\n",
        "\n",
        "      if self.activationFunc == \"sigmoid\":\n",
        "        activation = sigmoid(preActivation)\n",
        "      elif self.activationFunc == \"tanh\":\n",
        "        activation = _tanh(preActivation)\n",
        "      elif self.activationFunc == \"relu\":\n",
        "        activation = relu(preActivation)\n",
        "      y = activation\n",
        "\n",
        "    # last layer we don't need activation\n",
        "    y = preActivation\n",
        "    # doing softmax doing the each column wise\n",
        "    exp_y = np.exp(y - np.max(y, axis=0, keepdims=True))  # Improve numerical stability\n",
        "    y = exp_y / np.sum(exp_y, axis=0, keepdims=True)\n",
        "    predictions = []\n",
        "    for i in range(n):\n",
        "      y_pred = np.argmax(y[:,i])\n",
        "      predictions.append(y_pred)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "  def SGD(self):\n",
        "    W, B, preActivation, activation  = self.Initialise()\n",
        "    iteration = 0\n",
        "    layers = self.layers\n",
        "    empty_W, empty_B = self.InitialiseEmptyWeightsAndBiases()\n",
        "\n",
        "    while(iteration < self.epochs):\n",
        "      i = 0\n",
        "      while i < self.X_train.shape[1]:\n",
        "        y = self.FeedForward(self.X_train[:, i:i+self.batchSize], W, B, preActivation, activation)\n",
        "        # these are the partial derivates for one point\n",
        "        dw, db = self.BackWardPropogation(self.X_train[:, i:i+self.batchSize], self.Y_train[i:i+self.batchSize], W, preActivation, activation, y)\n",
        "\n",
        "        # we will update the weights now\n",
        "        for k in range(1, layers+1):\n",
        "            W[k] = W[k] - self.learningRate*dw[k]\n",
        "            B[k] = B[k] - self.learningRate*db[k]\n",
        "\n",
        "        i += self.batchSize\n",
        "      acuu, loss = self.FindAccuracyAndLoss(W, B, self.X_train, self.Y_train)\n",
        "      v_acc, v_loss = self.FindAccuracyAndLoss(W, B, self.X_val, self.Y_val)\n",
        "      if self.isWandb == True:\n",
        "        wandb.log({'accuracy': acuu})\n",
        "        wandb.log({'loss': loss})\n",
        "        wandb.log({'v_accuracy': v_acc})\n",
        "        wandb.log({'v_loss': v_loss})\n",
        "        wandb.log({'epoch': iteration})\n",
        "      print(acuu, loss, v_acc, v_loss)\n",
        "      iteration += 1\n",
        "\n",
        "    self.Weights = W\n",
        "    self.Baises  = B\n",
        "\n",
        "  def MomentBasedGradientDecent(self):\n",
        "    W, B, preActivation, activation  = self.Initialise()\n",
        "    iteration = 0\n",
        "    u_W, u_B = self.InitialiseEmptyWeightsAndBiases()\n",
        "    # inititialising u to be zero\n",
        "\n",
        "    while(iteration < self.epochs):\n",
        "      i = 0\n",
        "      while i < self.X_train.shape[1]:\n",
        "        # batch wise forward and backward passes\n",
        "        y = self.FeedForward(self.X_train[:, i:i+self.batchSize], W, B, preActivation, activation)\n",
        "        dw, db = self.BackWardPropogation(self.X_train[:, i:i+self.batchSize], self.Y_train[i:i+self.batchSize], W, preActivation, activation, y)\n",
        "\n",
        "        # update the momentum with the gradient\n",
        "        for k in range(1, self.layers+1):\n",
        "          u_W[k] = u_W[k]*self.betha + dw[k]\n",
        "          u_B[k] = u_B[k]*self.betha + db[k]\n",
        "\n",
        "        # we will update the weights now with the momentum\n",
        "        for k in range(1, self.layers+1):\n",
        "            W[k] = W[k] - self.learningRate*u_W[k]\n",
        "            B[k] = B[k] - self.learningRate*u_B[k]\n",
        "\n",
        "        # next batch\n",
        "        i += self.batchSize\n",
        "      acuu, loss = self.FindAccuracyAndLoss(W, B, self.X_train, self.Y_train)\n",
        "      v_acc, v_loss = self.FindAccuracyAndLoss(W, B, self.X_val, self.Y_val)\n",
        "      if self.isWandb == True:\n",
        "        wandb.log({'accuracy': acuu})\n",
        "        wandb.log({'loss': loss})\n",
        "        wandb.log({'v_accuracy': v_acc})\n",
        "        wandb.log({'v_loss': v_loss})\n",
        "        wandb.log({'epoch': iteration})\n",
        "      print(acuu, loss, v_acc, v_loss)\n",
        "      iteration += 1\n",
        "\n",
        "    self.Weights = W\n",
        "    self.Baises  = B\n",
        "\n",
        "  def NestrovBasedGradientDescent(self):\n",
        "    iteration = 0\n",
        "    W, B, preActivation, activation = self.Initialise()\n",
        "    u_W, u_B = self.InitialiseEmptyWeightsAndBiases()\n",
        "    # initializing u to be zero\n",
        "\n",
        "    while(iteration < self.epochs):\n",
        "      i = 0\n",
        "      while i < self.X_train.shape[1]:\n",
        "\n",
        "        y = self.FeedForward(self.X_train[:, i:i+self.batchSize], W, B, preActivation, activation)\n",
        "        dw, db = self.BackWardPropogation(self.X_train[:, i:i+self.batchSize], self.Y_train[i:i+self.batchSize], W, preActivation, activation, y)\n",
        "\n",
        "        for k in range(1, self.layers+1):\n",
        "            u_W[k] = u_W[k]*self.betha + dw[k]\n",
        "            u_B[k] = u_B[k]*self.betha + db[k]\n",
        "\n",
        "        for k in range(1, self.layers+1):\n",
        "            W[k] = W[k] - self.learningRate*(self.betha* u_W[k]+ dw[k])\n",
        "            B[k] = B[k] - self.learningRate*(self.betha* u_B[k]+ db[k])\n",
        "\n",
        "        i += self.batchSize\n",
        "      acuu, loss = self.FindAccuracyAndLoss(W, B, self.X_train, self.Y_train)\n",
        "      v_acc, v_loss = self.FindAccuracyAndLoss(W, B, self.X_val, self.Y_val)\n",
        "      if self.isWandb == True:\n",
        "        wandb.log({'accuracy': acuu})\n",
        "        wandb.log({'loss': loss})\n",
        "        wandb.log({'v_accuracy': v_acc})\n",
        "        wandb.log({'v_loss': v_loss})\n",
        "        wandb.log({'epoch': iteration})\n",
        "      print(acuu, loss, v_acc, v_loss)\n",
        "      iteration += 1\n",
        "\n",
        "    self.Weights = W\n",
        "    self.Baises  = B\n",
        "\n",
        "  def RMSPROP(self):\n",
        "    iteration = 0\n",
        "    epochs = self.epochs\n",
        "    layers = self.layers\n",
        "    batchSize = self.batchSize\n",
        "    betha = self.betha\n",
        "    W, B, preActivation, activation  = self.Initialise()\n",
        "    v_W, v_B = self.InitialiseEmptyWeightsAndBiases()\n",
        "    # inititialising u to be zero\n",
        "\n",
        "    while(iteration < epochs):\n",
        "      i = 0\n",
        "      while i < self.X_train.shape[1]:\n",
        "        y = self.FeedForward(self.X_train[:, i:i+batchSize], W, B, preActivation, activation)\n",
        "        dw, db = self.BackWardPropogation(self.X_train[:, i:i+batchSize], self.Y_train[i:i+batchSize], W, preActivation, activation, y)\n",
        "\n",
        "        # update the v values with the gradient\n",
        "        for k in range(1, layers+1):\n",
        "          v_W[k] = v_W[k]*betha + (1 - betha) * (dw[k] ** 2)\n",
        "          v_B[k] = v_B[k]*betha + (1 - betha) * (db[k] ** 2)\n",
        "\n",
        "        # we will update the weights now with the momentum\n",
        "        for k in range(1, layers+1):\n",
        "          W[k] = W[k] - (self.learningRate/np.sqrt(v_W[k] + self.epsilon))*dw[k]\n",
        "          B[k] = B[k] - (self.learningRate/np.sqrt(v_B[k] + self.epsilon))*db[k]\n",
        "\n",
        "        i += batchSize\n",
        "      acuu, loss = self.FindAccuracyAndLoss(W, B, self.X_train, self.Y_train)\n",
        "      v_acc, v_loss = self.FindAccuracyAndLoss(W, B, self.X_val, self.Y_val)\n",
        "      if self.isWandb == True:\n",
        "        wandb.log({'accuracy': acuu})\n",
        "        wandb.log({'loss': loss})\n",
        "        wandb.log({'v_accuracy': v_acc})\n",
        "        wandb.log({'v_loss': v_loss})\n",
        "        wandb.log({'epoch': iteration})\n",
        "      print(acuu, loss, v_acc, v_loss)\n",
        "      iteration += 1\n",
        "\n",
        "    self.Weights = W\n",
        "    self.Baises  = B\n",
        "\n",
        "  def ADAM(self):\n",
        "    iteration = 0\n",
        "    epochs = self.epochs\n",
        "    layers = self.layers\n",
        "    batchSize = self.batchSize\n",
        "\n",
        "    W, B, preActivation, activation  = self.Initialise()\n",
        "    v_W, v_B = self.InitialiseEmptyWeightsAndBiases()\n",
        "    m_W, m_B = self.InitialiseEmptyWeightsAndBiases()\n",
        "    mhat_W, mhat_B = self.InitialiseEmptyWeightsAndBiases()\n",
        "    vhat_W, vhat_B = self.InitialiseEmptyWeightsAndBiases()\n",
        "    # inititialising u to be zero\n",
        "    t = 1\n",
        "\n",
        "    while(iteration < epochs):\n",
        "      # this is used to compute the gradients\n",
        "      i = 0\n",
        "      while i < self.X_train.shape[1]:\n",
        "        y = self.FeedForward(self.X_train[:, i:i+batchSize], W, B, preActivation, activation)\n",
        "        dw, db = self.BackWardPropogation(self.X_train[:, i:i+batchSize], self.Y_train[i:i+batchSize], W, preActivation, activation, y)\n",
        "\n",
        "        # updating the momentum\n",
        "        for k in range(1, layers+1):\n",
        "          m_W[k] = self.betha1*m_W[k] + (1 - self.betha1)*dw[k]\n",
        "          m_B[k] = self.betha1*m_B[k] + (1 - self.betha1)*db[k]\n",
        "\n",
        "          # finding m hat of W and B\n",
        "          mhat_W[k] = m_W[k]/(1 - self.betha1 ** t)\n",
        "          mhat_B[k] = m_B[k]/(1 - self.betha1 ** t)\n",
        "\n",
        "        # update the v values with the gradient\n",
        "        for k in range(1, layers+1):\n",
        "          v_W[k] = v_W[k]*self.betha2 + (1 - self.betha2) * (dw[k] ** 2)\n",
        "          v_B[k] = v_B[k]*self.betha2 + (1 - self.betha2) * (db[k] ** 2)\n",
        "\n",
        "          # finding v hat of W and B\n",
        "          vhat_W[k] = v_W[k]/(1 - self.betha2 ** t)\n",
        "          vhat_B[k] = v_B[k]/(1 - self.betha2 ** t)\n",
        "\n",
        "        # we will update the weights now with the momentum\n",
        "        for k in range(1, layers+1):\n",
        "          l2_norm_w = np.linalg.norm(vhat_W[k])\n",
        "          l2_norm_b = np.linalg.norm(vhat_B[k])\n",
        "          W[k] = W[k] - (self.learningRate/np.sqrt(l2_norm_w) + self.epsilon)*mhat_W[k]\n",
        "          B[k] = B[k] - (self.learningRate/np.sqrt(l2_norm_b) + self.epsilon)*mhat_B[k]\n",
        "\n",
        "        t += 1\n",
        "        i += self.batchSize\n",
        "\n",
        "      acuu, loss = self.FindAccuracyAndLoss(W, B, self.X_train, self.Y_train)\n",
        "      v_acc, v_loss = self.FindAccuracyAndLoss(W, B, self.X_val, self.Y_val)\n",
        "      if self.isWandb == True:\n",
        "        wandb.log({'accuracy': acuu})\n",
        "        wandb.log({'loss': loss})\n",
        "        wandb.log({'v_accuracy': v_acc})\n",
        "        wandb.log({'v_loss': v_loss})\n",
        "        wandb.log({'epoch': iteration})\n",
        "      print(acuu, loss, v_acc, v_loss)\n",
        "      iteration += 1\n",
        "\n",
        "    self.Weights = W\n",
        "    self.Baises  = B\n",
        "\n",
        "  def NADAM(self):\n",
        "    iteration = 0\n",
        "    epochs = self.epochs\n",
        "    layers = self.layers\n",
        "    W, B, preActivation, activation  = self.Initialise()\n",
        "    v_W, v_B = self.InitialiseEmptyWeightsAndBiases()\n",
        "    m_W, m_B = self.InitialiseEmptyWeightsAndBiases()\n",
        "    mhat_W, mhat_B = self.InitialiseEmptyWeightsAndBiases()\n",
        "    vhat_W, vhat_B = self.InitialiseEmptyWeightsAndBiases()\n",
        "    # inititialising u to be zero\n",
        "    t = 1\n",
        "\n",
        "    while(iteration < self.epochs):\n",
        "      # this is used to compute the gradients\n",
        "      i = 0\n",
        "      while i < self.X_train.shape[1]:\n",
        "        y = self.FeedForward(self.X_train[:, i:i+self.batchSize], W, B, preActivation, activation)\n",
        "        dw, db = self.BackWardPropogation(self.X_train[:, i:i+self.batchSize], self.Y_train[i:i+self.batchSize], W, preActivation, activation, y)\n",
        "\n",
        "        # updating the momentum\n",
        "        for k in range(1, layers+1):\n",
        "          m_W[k] = self.betha1*m_W[k] + (1 - self.betha1)*dw[k]\n",
        "          m_B[k] = self.betha1*m_B[k] + (1 - self.betha1)*db[k]\n",
        "\n",
        "          # finding m hat of W and B\n",
        "          mhat_W[k] = m_W[k]/(1 - self.betha1 ** t)\n",
        "          mhat_B[k] = m_B[k]/(1 - self.betha1 ** t)\n",
        "\n",
        "        # update the v values with the gradient\n",
        "        for k in range(1, layers+1):\n",
        "          v_W[k] = v_W[k]*self.betha2 + (1 - self.betha2) * (dw[k] ** 2)\n",
        "          v_B[k] = v_B[k]*self.betha2 + (1 - self.betha2) * (db[k] ** 2)\n",
        "\n",
        "          # finding v hat of W and B\n",
        "          vhat_W[k] = v_W[k]/(1 - self.betha2 ** t)\n",
        "          vhat_B[k] = v_B[k]/(1 - self.betha2 ** t)\n",
        "\n",
        "        # we will update the weights now with the momentum\n",
        "        for k in range(1, layers+1):\n",
        "          l2_norm_w = np.linalg.norm(vhat_W[k])\n",
        "          l2_norm_b = np.linalg.norm(vhat_B[k])\n",
        "          W[k] = W[k] - (self.learningRate/np.sqrt(l2_norm_w) + self.epsilon)*(mhat_W[k]*self.betha1 + (1 - self.betha1)*dw[k]/(1 - self.betha1 ** t))\n",
        "          B[k] = B[k] - (self.learningRate/np.sqrt(l2_norm_b) + self.epsilon)*(mhat_B[k]*self.betha1 + (1 - self.betha1)*db[k]/(1 - self.betha1 ** t))\n",
        "\n",
        "        t += 1\n",
        "        i += self.batchSize\n",
        "\n",
        "      acuu, loss = self.FindAccuracyAndLoss(W, B, self.X_train, self.Y_train)\n",
        "      v_acc, v_loss = self.FindAccuracyAndLoss(W, B, self.X_val, self.Y_val)\n",
        "      if self.isWandb == True:\n",
        "        wandb.log({'accuracy': acuu})\n",
        "        wandb.log({'loss': loss})\n",
        "        wandb.log({'v_accuracy': v_acc})\n",
        "        wandb.log({'v_loss': v_loss})\n",
        "        wandb.log({'epoch': iteration})\n",
        "      print(acuu, loss, v_acc, v_loss)\n",
        "      iteration += 1\n",
        "\n",
        "    self.Weights = W\n",
        "    self.Baises  = B\n",
        "\n",
        "  def fit(self):\n",
        "    if self.optimiser == \"sgd\":\n",
        "      self.SGD()\n",
        "    if self.optimiser == \"momentum\":\n",
        "      self.MomentBasedGradientDecent()\n",
        "    if self.optimiser == \"nestrov\":\n",
        "      self.NestrovBasedGradientDescent()\n",
        "    if self.optimiser == \"rmsprop\":\n",
        "      self.RMSPROP()\n",
        "    if self.optimiser == \"adam\":\n",
        "      self.ADAM()\n",
        "    if self.optimiser == \"nadam\":\n",
        "      self.NADAM()\n",
        "\n",
        "  def confusionMatrix(self):\n",
        "    # on the test data set\n",
        "    predictions = self.predict(self.X_test)\n",
        "    if self.dataset == \"fashion_mnist\":\n",
        "      class_names = [\"T-shirt/Top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle Boot\"]\n",
        "    else:\n",
        "      class_names = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
        "\n",
        "    if self.isWandb == True:\n",
        "      conf_matrix = confusion_matrix(self.Y_test, predictions)\n",
        "      plt.figure(figsize=(10, 7))\n",
        "      sns_heatmap = sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "                                xticklabels=class_names, yticklabels=class_names)\n",
        "      plt.title('Confusion Matrix')\n",
        "      plt.ylabel('True Label')\n",
        "      plt.xlabel('Predicted Label')\n",
        "\n",
        "      # Save the plot to an image file\n",
        "      heatmap_image_filename = \"confusion_matrix_heatmap.png\"\n",
        "      plt.savefig(heatmap_image_filename)\n",
        "      plt.close()  # Close the plot to avoid displaying it in the notebook/output\n",
        "\n",
        "      # Log the image to Wandb\n",
        "      wandb.log({\"confusion_matrix_custom\": wandb.Image(heatmap_image_filename)})\n",
        "\n",
        "    else:\n",
        "      conf_matrix = confusion_matrix(self.Y_test, predictions)\n",
        "      plt.figure(figsize=(10, 7))\n",
        "      sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "                  xticklabels= class_names,\n",
        "                  yticklabels= class_names)\n",
        "      plt.title('Confusion Matrix')\n",
        "      plt.ylabel('True Label')\n",
        "      plt.xlabel('Predicted Label')\n",
        "      plt.show()\n"
      ],
      "metadata": {
        "id": "0Lw_kRquTyVE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "id": "io6UukPcT-v5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetwork(inputSize = 784, hiddenLayers = 4, outputSize = 10, sizeOfHiddenLayers = 64, batchSize = 32, learningRate = 0.001, initialisationType = \"Xavier\", optimiser = \"nadam\", activationFunc=\"relu\",weightDecay = 0.0005,lossFunc = \"MSE\", epochs = 10, dataset = \"mnist\")\n",
        "model.fit()"
      ],
      "metadata": {
        "id": "fOQOUETYUSi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetwork(inputSize = 784, hiddenLayers = 4, outputSize = 10, sizeOfHiddenLayers = 128, batchSize = 64, learningRate = 0.001, initialisationType = \"Xavier\", optimiser = \"rmsprop\", activationFunc=\"relu\",weightDecay = 0.0005,lossFunc = \"cross_entropy\", epochs = 10, dataset = \"mnist\")\n",
        "model.fit()"
      ],
      "metadata": {
        "id": "kw6SYFCmcV16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.confusionMatrix()"
      ],
      "metadata": {
        "id": "FX1-Is5wich0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for printing the confusion matrix\n",
        "import wandb\n",
        "\n",
        "# Define the sweep configuration\n",
        "sweep_config = {\n",
        "    'method': 'grid',\n",
        "    'name' : 'cross entropy and MSE',\n",
        "    'metric': {\n",
        "      'name': 'accuracy',\n",
        "      'goal': 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'lossFunction': {\n",
        "            'values': [\"cross_entropy\", \"MSE\"]\n",
        "        },\n",
        "        # Define other parameters here\n",
        "    }\n",
        "}\n",
        "def main():\n",
        "    # Initialize a wandb run\n",
        "    wandb.init()\n",
        "    # Access sweep parameters via wandb.config\n",
        "    config = wandb.config\n",
        "    run_name = f\"{config.lossFunction}_momentum_tanh_Xavier_4_128\"\n",
        "\n",
        "    # Set the run name\n",
        "    wandb.run.name = run_name\n",
        "    wandb.run.save()\n",
        "\n",
        "\n",
        "    # Define and train the model using parameters from config\n",
        "    model = NeuralNetwork(inputSize=784, hiddenLayers=4, outputSize=10,\n",
        "                          sizeOfHiddenLayers=128, batchSize=32,\n",
        "                          learningRate=0.0001, initialisationType=\"Xavier\",\n",
        "                          optimiser=\"momentum\", activationFunc=\"tanh\", weightDecay=0.0005,\n",
        "                          epochs=10, isWandb=True, lossFunc = config.lossFunction)\n",
        "    model.fit()\n",
        "\n",
        "    wandb.finish()\n",
        "\n",
        "# Create the sweep\n",
        "sweep_id = wandb.sweep(sweep=sweep_config, project='Assignment 1')\n",
        "\n",
        "# Start the sweep agent\n",
        "wandb.agent(sweep_id, main, count=2)\n"
      ],
      "metadata": {
        "id": "jLYTX3P8nBRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for printing the confusion matrix\n",
        "import wandb\n",
        "\n",
        "# Define the sweep configuration\n",
        "sweep_config = {\n",
        "    'method': 'bayes',\n",
        "    'name' : 'confusion matrix',\n",
        "    'metric': {\n",
        "      'name': 'accuracy',\n",
        "      'goal': 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'batchSize': {\n",
        "            'values': [32]\n",
        "        },\n",
        "        # Define other parameters here\n",
        "    }\n",
        "}\n",
        "def main():\n",
        "    # Initialize a wandb run\n",
        "    wandb.init()\n",
        "    # Access sweep parameters via wandb.config\n",
        "    config = wandb.config\n",
        "\n",
        "    # Define and train the model using parameters from config\n",
        "    model = NeuralNetwork(inputSize=784, hiddenLayers=4, outputSize=10,\n",
        "                          sizeOfHiddenLayers=64, batchSize=config.batchSize,\n",
        "                          learningRate=0.0001, initialisationType=\"Xavier\",\n",
        "                          optimiser=\"nadam\", activationFunc=\"relu\", weightDecay=0.0005,\n",
        "                          epochs=2, isWandb=True)\n",
        "    model.fit()\n",
        "    model.confusionMatrix()\n",
        "\n",
        "    wandb.finish()\n",
        "\n",
        "# Create the sweep\n",
        "sweep_id = wandb.sweep(sweep=sweep_config, project='Assignment 1')\n",
        "\n",
        "# Start the sweep agent\n",
        "wandb.agent(sweep_id, main, count=1)\n"
      ],
      "metadata": {
        "id": "_CjbFUx0WuaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    wandb.init(project=\"Assignment 1\")\n",
        "    config = wandb.config\n",
        "    run_name = f\"{config.optimiser}_{config.activation}_{config.hidden_layers}_{config.hidden_layer_size}_{config.batch_size}\"\n",
        "\n",
        "    # Set the run name\n",
        "    wandb.run.name = run_name\n",
        "    wandb.run.save()\n",
        "\n",
        "    # Define and train the model as before\n",
        "    model = NeuralNetwork(inputSize = 784, hiddenLayers = config.hidden_layers, outputSize = 10, sizeOfHiddenLayers = config.hidden_layer_size, batchSize = config.batch_size, learningRate = config.learning_rate, initialisationType = config.weights_initialisation, optimiser = config.optimiser, activationFunc=config.activation, epochs = config.epochs,weightDecay = config.weight_decay, isWandb = True)\n",
        "    model.fit()\n",
        "    wandb.finish()\n",
        "\n",
        "sweep_config = {\n",
        "    'method': 'bayes',\n",
        "    'name' : 'sweep cross entropy',\n",
        "    'metric': {\n",
        "      'name': 'v_accuracy',\n",
        "      'goal': 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'epochs': {\n",
        "            'values': [5,10]\n",
        "        },\n",
        "        'hidden_layers': {\n",
        "          'values': [3, 4, 5]\n",
        "        },\n",
        "        'hidden_layer_size':{\n",
        "            'values':[32,64,128]\n",
        "        },\n",
        "        'weight_decay': {\n",
        "            'values':[0, 0.0005, 0.5]\n",
        "        },\n",
        "        'batch_size': {\n",
        "            'values': [16, 32, 64]\n",
        "        },\n",
        "        'activation': {\n",
        "            'values': ['sigmoid','relu','tanh']\n",
        "        },\n",
        "        'optimiser': {\n",
        "            'values': ['sgd', 'momentum', 'nestrov', 'rmsprop', 'adam', 'nadam']\n",
        "        },\n",
        "        'weights_initialisation': {\n",
        "            'values': ['random', 'Xavier']\n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'values':[1e-2,1e-3,1e-4]\n",
        "        },\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "sweep_id = wandb.sweep(sweep=sweep_config,project='Assignment 1')\n",
        "wandb.agent(\"3ncb86iq\" , function = main , count = 400)\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "oZN37oRFUCNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QgDv_rbBUc41"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}