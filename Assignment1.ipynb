{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYFE_FjORfmY"
      },
      "source": [
        "Fashion MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nzOH8J49_z_",
        "outputId": "141901e0-69d4-47cf-dca5-6ca2d17479af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.16.4-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.42.0-py2.py3-none-any.whl (263 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.5/263.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.42 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.42.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.4\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "88B3FZLgROCd"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "q7T1Yop-VxNA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61953c25-e3cd-4c25-db93-78fd78269258"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "!wandb login\n",
        "\n",
        "#338ff25d87248e7f3e86e98c746e32fe09553c9e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4wHMH8dR8y3"
      },
      "outputs": [],
      "source": [
        "(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBhHii5mPEpL",
        "outputId": "77112b04-05f2-4b05-d862-21fe2145a91a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(48000, 28, 28)\n",
            "(10000, 28, 28)\n",
            "(12000, 28, 28)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(X_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 636
        },
        "id": "KYAo8ZQzSSA2",
        "outputId": "69cbc570-29c7-452f-95a5-51e8e0d01750"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# this array keeps track for ind of each class\n",
        "images_ind = []\n",
        "# classes which are present\n",
        "class_names = [\"T-shirt/Top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle Boot\"]\n",
        "\n",
        "for i in range(10):\n",
        "  for ind in range(X_train.shape[0]):\n",
        "    if Y_train[ind] == i:\n",
        "      images_ind.append(ind)\n",
        "      break\n",
        "\n",
        "# print(images_ind)\n",
        "\n",
        "figure, axes = plt.subplots(2, 5, figsize=(10, 10))\n",
        "\n",
        "for i, ax in enumerate(axes.flat):\n",
        "  ind = images_ind[i]\n",
        "  ax.imshow(X_train[ind], cmap='gray')\n",
        "  ax.axis('off')\n",
        "  ax.set_title(class_names[i])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()  # Display the plot"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# normalsing and resisizing all the images\n",
        "X_train = X_train/255.0\n",
        "X_test  = X_test/255.0\n",
        "X_val   = X_val/255.0\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], 784).T\n",
        "X_test = X_test.reshape(X_test.shape[0], 784).T\n",
        "X_val = X_val.reshape(X_val.shape[0], 784).T\n",
        "print(X_train.shape[0])\n"
      ],
      "metadata": {
        "id": "YlGrK71y-Xcd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36983163-1b8c-434a-fd7f-749e26484c88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "784\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vp_K0iYCegvc"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    clip_x = np.clip(x, -500, 500)  # Cliping x to avoid overflow\n",
        "    return 1 / (1 + np.exp(-clip_x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5knxthLglqc"
      },
      "outputs": [],
      "source": [
        "def Initialise(Layers, LayerWise, InputSize, OutputSize):\n",
        "  # initialising weights and biases as a key value pair\n",
        "  W = {}\n",
        "  B = {}\n",
        "\n",
        "  PreActivation = {}\n",
        "  Activation = {}\n",
        "\n",
        "  # adding input layer\n",
        "  LayerWise.insert(0, InputSize)\n",
        "  for i in range(Layers):\n",
        "    W[i+1] = 0.01*np.random.randn(LayerWise[i+1], LayerWise[i])\n",
        "    B[i+1] = 0.01*np.random.randn(LayerWise[i+1], 1)\n",
        "\n",
        "    # preactivation and activation will have same size\n",
        "    PreActivation[i+1] = np.zeros((LayerWise[i+1], 1))\n",
        "    Activation[i+1]    = np.zeros((LayerWise[i+1], 1))\n",
        "  del LayerWise[0]\n",
        "\n",
        "  return W,B,PreActivation,Activation\n",
        "\n",
        "def InitialiseEmptyWeightsAndBiases(Layers, LayerWise, InputSize):\n",
        "  W = {}\n",
        "  B = {}\n",
        "  LayerWise.insert(0, InputSize)\n",
        "  for i in range(Layers):\n",
        "    W[i+1] = np.zeros((LayerWise[i+1], LayerWise[i]))\n",
        "    B[i+1] = np.zeros((LayerWise[i+1], 1))\n",
        "  del LayerWise[0]\n",
        "\n",
        "  return W,B\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8Y2SUctr2Jj"
      },
      "outputs": [],
      "source": [
        "def FeedForward(x, W, B, preActivation, activation):\n",
        "  # no of layers\n",
        "  n = len(W)\n",
        "  # here x can be list of elements at a go\n",
        "  # softmax function\n",
        "  y = x\n",
        "  for i in range(1, n+1):\n",
        "    preActivation[i] = np.dot(W[i], y) + B[i]\n",
        "    activation[i] = sigmoid(preActivation[i])\n",
        "    y = activation[i]\n",
        "\n",
        "  # last layer we don't need activation\n",
        "  y = preActivation[n]\n",
        "  # doing softmax\n",
        "  # doing the each column wise\n",
        "  exp_y = np.exp(y - np.max(y, axis=0, keepdims=True))  # Improve numerical stability\n",
        "  y = exp_y / np.sum(exp_y, axis=0, keepdims=True)\n",
        "  return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYN3-nH5whWZ"
      },
      "outputs": [],
      "source": [
        "def BackWardPropogation(X, y_corr, W, preActivation, activation, y_hat):\n",
        "  # y_hat is the prediction\n",
        "  # y_corr is the correct class\n",
        "\n",
        "  dw = {}\n",
        "  db = {}\n",
        "\n",
        "  # these many points are there in the batch\n",
        "  batchSize = y_corr.shape[0]\n",
        "  da_l = y_hat\n",
        "\n",
        "  ind = 0\n",
        "  while ind < batchSize:\n",
        "    da_l[y_corr[ind]][ind] -= 1\n",
        "    ind += 1\n",
        "  da = da_l\n",
        "\n",
        "\n",
        "  activation[0] = X\n",
        "  layer = len(W)\n",
        "  dh = da #used for finding next layer\n",
        "  while layer >= 1:\n",
        "    dw[layer] = np.dot(da, activation[layer-1].T)\n",
        "    db[layer] = da.sum(axis=1, keepdims=True)\n",
        "    if layer > 1:\n",
        "      dh = np.dot(W[layer].T, da)\n",
        "      dg = activation[layer-1] * (1 - activation[layer-1])\n",
        "      da = dh * dg\n",
        "      # hedamant product\n",
        "\n",
        "    layer -= 1\n",
        "\n",
        "\n",
        "  return dw, db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9uwy4wQu8iS"
      },
      "outputs": [],
      "source": [
        "def findLoss(W, B, preActivation, activation):\n",
        "  loss = 0\n",
        "  for i in range(X_train.shape[1]):\n",
        "    y = FeedForward(X_train[:,i, np.newaxis], W, B, preActivation, activation)\n",
        "    # y returns the vector of the probabilities\n",
        "    # y_train[i] is the correct prediction of the data\n",
        "    loss = loss -1*math.log(y[Y_train[i]])\n",
        "\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5zCb-qbHG9U"
      },
      "outputs": [],
      "source": [
        "def FindAccuracy(W, B, preActivation, activation):\n",
        "  n = X_test.shape[1]\n",
        "  correct = 0\n",
        "  y = FeedForward(X_train, W, B, preActivation, activation)\n",
        "\n",
        "  for i in range(n):\n",
        "    y_pred = np.argmax(y[:,i])\n",
        "    if Y_train[i] == y_pred:\n",
        "      correct += 1\n",
        "\n",
        "  return (correct*100/ n)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwwIyzSBz8eS"
      },
      "outputs": [],
      "source": [
        "def gradient_decent(epochs, learningRate):\n",
        "  layers = 3\n",
        "  nodes  = [128, 64, 10]\n",
        "  inputSize  = 28*28\n",
        "  outputSize = 10\n",
        "  W, B, preActivation, activation  = Initialise(layers, nodes, inputSize, outputSize)\n",
        "  iteration = 0\n",
        "\n",
        "\n",
        "  while(iteration < epochs):\n",
        "    dW, dB = InitialiseEmptyWeightsAndBiases(layers, nodes, inputSize)\n",
        "\n",
        "    for i in range(X_train.shape[1]):\n",
        "      y = FeedForward(X_train[:,i, np.newaxis], W, B, preActivation, activation)\n",
        "      # these are the partial derivates for one point\n",
        "      dw, db = BackWardPropogation(X_train[:,i,np.newaxis], Y_train[i], W, preActivation, activation, y)\n",
        "\n",
        "      # we need to update this for every layer\n",
        "      for j in range(1, layers+1):\n",
        "          dW[j] = dW[j] + dw[j]\n",
        "          dB[j] = dB[j] + db[j]\n",
        "\n",
        "\n",
        "    # overall updation\n",
        "    # updating weights and baises\n",
        "    for i in range(1, layers+1):\n",
        "        W[i] = W[i] - learningRate*dW[i]\n",
        "        B[i] = B[i] - learningRate*dB[i]\n",
        "\n",
        "    # if iteration%5 == 0:\n",
        "    print(FindAccuracy(W, B, preActivation, activation))\n",
        "    iteration += 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Btq6OjgHOFaz",
        "outputId": "d3e596cf-eb49-4568-b303-1129d159f96c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9.09\n",
            "8.74\n",
            "9.19\n",
            "9.51\n",
            "9.98\n",
            "10.33\n",
            "10.63\n",
            "10.89\n",
            "11.22\n",
            "11.58\n",
            "12.03\n",
            "12.39\n"
          ]
        }
      ],
      "source": [
        "gradient_decent(12, 0.00001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6szaHgG-nQya"
      },
      "outputs": [],
      "source": [
        "def StochasticGradientDecent(epochs, learningRate, batchSize):\n",
        "  layers = 3\n",
        "  nodes  = [128, 64, 10]\n",
        "  inputSize  = 28*28\n",
        "  outputSize = 10\n",
        "  W, B, preActivation, activation  = Initialise(layers, nodes, inputSize, outputSize)\n",
        "  iteration = 0\n",
        "  empty_W, empty_B = InitialiseEmptyWeightsAndBiases(layers, nodes, inputSize)\n",
        "\n",
        "  while(iteration < epochs):\n",
        "    updates_W, updates_B = InitialiseEmptyWeightsAndBiases(layers, nodes, inputSize)\n",
        "    i = 0\n",
        "    while i < X_train.shape[1]:\n",
        "      y = FeedForward(X_train[:, i:i+batchSize], W, B, preActivation, activation)\n",
        "      # these are the partial derivates for one point\n",
        "      dw, db = BackWardPropogation(X_train[:, i:i+batchSize], Y_train[i:i+batchSize], W, preActivation, activation, y)\n",
        "\n",
        "      # we need to update this for every layer\n",
        "      for j in range(1, layers+1):\n",
        "          updates_W[j] = updates_W[j] + dw[j]\n",
        "          updates_B[j] = updates_B[j] + db[j]\n",
        "\n",
        "        # we will update the weights now\n",
        "      for k in range(1, layers+1):\n",
        "          W[k] = W[k] - learningRate*updates_W[k]\n",
        "          B[k] = B[k] - learningRate*updates_B[k]\n",
        "\n",
        "      # reinitiallising them to zeros\n",
        "      updates_W, updates_B = InitialiseEmptyWeightsAndBiases(layers, nodes, inputSize)\n",
        "      i += batchSize\n",
        "\n",
        "    print(FindAccuracy(W, B, preActivation, activation))\n",
        "    iteration += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfH_W9XTnQvJ",
        "outputId": "ed669ee7-8e59-49f9-e603-1c1dbf53f3d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9.74\n",
            "9.74\n",
            "26.97\n",
            "44.76\n",
            "61.71\n",
            "65.79\n",
            "71.14\n",
            "74.35\n",
            "76.09\n",
            "77.38\n"
          ]
        }
      ],
      "source": [
        "StochasticGradientDecent(10, 0.001, 2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "StochasticGradientDecent(10, 0.001, 16)"
      ],
      "metadata": {
        "id": "EQh198pFZLfy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3878798d-23a7-4133-f4dc-1ee1bead2356"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.9\n",
            "9.74\n",
            "28.61\n",
            "48.24\n",
            "60.94\n",
            "65.48\n",
            "70.58\n",
            "73.62\n",
            "75.88\n",
            "77.44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def MomentBasedGradientDecent(epochs, learningRate, batchSize, betha):\n",
        "  layers = 3\n",
        "  nodes  = [128, 64, 10]\n",
        "  inputSize  = 28*28\n",
        "  outputSize = 10\n",
        "  W, B, preActivation, activation  = Initialise(layers, nodes, inputSize, outputSize)\n",
        "\n",
        "  iteration = 0\n",
        "  u_W, u_B = InitialiseEmptyWeightsAndBiases(layers, nodes, inputSize)\n",
        "  # inititialising u to be zero\n",
        "\n",
        "  while(iteration < epochs):\n",
        "    updates_W, updates_B = InitialiseEmptyWeightsAndBiases(layers, nodes, inputSize)\n",
        "    i = 0\n",
        "    while i < X_train.shape[1]:\n",
        "      # batch wise forward and backward passes\n",
        "      y = FeedForward(X_train[:, i:i+batchSize], W, B, preActivation, activation)\n",
        "      dw, db = BackWardPropogation(X_train[:, i:i+batchSize], Y_train[i:i+batchSize], W, preActivation, activation, y)\n",
        "\n",
        "      # update the momentum with the gradient\n",
        "      for k in range(1, layers+1):\n",
        "        u_W[k] = u_W[k]*betha + dw[k]\n",
        "        u_B[k] = u_B[k]*betha + db[k]\n",
        "\n",
        "      # we will update the weights now with the momentum\n",
        "      for k in range(1, layers+1):\n",
        "          W[k] = W[k] - learningRate*u_W[k]\n",
        "          B[k] = B[k] - learningRate*u_B[k]\n",
        "\n",
        "      # next batch\n",
        "      i += batchSize\n",
        "\n",
        "    print(FindAccuracy(W, B, preActivation, activation))\n",
        "    iteration += 1"
      ],
      "metadata": {
        "id": "B1EW6wDy6VsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MomentBasedGradientDecent(2, 0.001, 16, 0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMFeB6DU6Vl_",
        "outputId": "374bd4e7-8122-4ab8-939f-98baa88b7b86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "53.42\n",
            "73.71\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qx12m1oNVzcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NestrovBasedGradientDescent(epochs, learningRate, batchSize, beta):\n",
        "  layers = 4\n",
        "  nodes = [128, 128, 128, 10]\n",
        "  inputSize = 28*28\n",
        "  outputSize = 10\n",
        "  W, B, preActivation, activation = Initialise(layers, nodes, inputSize, outputSize)\n",
        "  iteration = 0\n",
        "  u_W, u_B = InitialiseEmptyWeightsAndBiases(layers, nodes, inputSize)\n",
        "  # initializing u to be zero\n",
        "\n",
        "  while(iteration < epochs):\n",
        "    i = 0\n",
        "    while i < X_train.shape[1]:\n",
        "\n",
        "      y = FeedForward(X_train[:, i:i+batchSize], W, B, preActivation, activation)\n",
        "      dw, db = BackWardPropogation(X_train[:, i:i+batchSize], Y_train[i:i+batchSize], W, preActivation, activation, y)\n",
        "\n",
        "      for k in range(1, layers+1):\n",
        "          u_W[k] = u_W[k]*beta + dw[k]\n",
        "          u_B[k] = u_B[k]*beta + db[k]\n",
        "\n",
        "      for k in range(1, layers+1):\n",
        "          W[k] = W[k] - learningRate*(beta* u_W[k]+ dw[k])\n",
        "          B[k] = B[k] - learningRate*(beta* u_B[k]+ db[k])\n",
        "\n",
        "      i += batchSize\n",
        "\n",
        "    print(FindAccuracy(W, B, preActivation, activation))\n",
        "    iteration += 1\n"
      ],
      "metadata": {
        "id": "7T5I4-MSA3p1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NestrovBasedGradientDescent(5, 0.001, 16, 0.9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqfoVvkMla39",
        "outputId": "d9d4b6b8-1707-4770-b71b-09e6f5342cfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20.19\n",
            "45.55\n",
            "61.73\n",
            "70.29\n",
            "75.45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NestrovBasedGradientDescent(10, 0.0001, 16, 0.9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmtyS4MqMCXo",
        "outputId": "81a37c71-8f8d-48c8-f1b9-aa7b06b5001f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.22\n",
            "10.22\n",
            "10.22\n",
            "10.42\n",
            "15.01\n",
            "9.5\n",
            "9.42\n",
            "9.43\n",
            "9.42\n",
            "9.42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RMSPROP updation rule\n",
        "$$v_t = \\beta v_{t-1} + (1 - \\beta)\\nabla w_t^2$$\n",
        "$$w_{t+1} = w_t - \\frac{\\eta}{\\sqrt{v_t + \\epsilon}} \\nabla w_t$$\n",
        "\n"
      ],
      "metadata": {
        "id": "IwQvhBeETOdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def RMSPROP(epochs, learningRate, batchSize, betha, epsioln):\n",
        "  layers = 3\n",
        "  nodes  = [128, 128, 128, 10]\n",
        "  inputSize  = 28*28\n",
        "  outputSize = 10\n",
        "  iteration = 0\n",
        "\n",
        "  W, B, preActivation, activation  = Initialise(layers, nodes, inputSize, outputSize)\n",
        "  v_W, v_B = InitialiseEmptyWeightsAndBiases(layers, nodes, inputSize)\n",
        "  # inititialising u to be zero\n",
        "\n",
        "  while(iteration < epochs):\n",
        "    i = 0\n",
        "    while i < X_train.shape[1]:\n",
        "      y = FeedForward(X_train[:, i:i+batchSize], W, B, preActivation, activation)\n",
        "      dw, db = BackWardPropogation(X_train[:, i:i+batchSize], Y_train[i:i+batchSize], W, preActivation, activation, y)\n",
        "\n",
        "      # update the v values with the gradient\n",
        "      for k in range(1, layers+1):\n",
        "        v_W[k] = v_W[k]*betha + (1 - betha) * (dw[k] ** 2)\n",
        "        v_B[k] = v_B[k]*betha + (1 - betha) * (db[k] ** 2)\n",
        "\n",
        "      # we will update the weights now with the momentum\n",
        "      for k in range(1, layers+1):\n",
        "        W[k] = W[k] - (learningRate/np.sqrt(v_W[k] + epsioln))*dw[k]\n",
        "        B[k] = B[k] - (learningRate/np.sqrt(v_B[k] + epsioln))*db[k]\n",
        "\n",
        "      i += batchSize\n",
        "\n",
        "    print(FindAccuracy(W, B, preActivation, activation))\n",
        "    iteration += 1"
      ],
      "metadata": {
        "id": "iSPR-OjcMCRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RMSPROP(8, 0.001, 16, 0.9, 0.005)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2wfl9SfpzYB",
        "outputId": "4be90e96-1581-4916-c624-49b397804535"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "51.8\n",
            "77.19\n",
            "83.34\n",
            "85.02\n",
            "86.0\n",
            "86.72\n",
            "87.27\n",
            "87.61\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RMSPROP(10, 0.0001, 64, 0.9, 0.0005)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1Uhe0rsCvHv",
        "outputId": "454e5112-6419-4442-e286-f42aaef5846e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9.74\n",
            "21.52\n",
            "36.86\n",
            "42.52\n",
            "47.53\n",
            "53.33\n",
            "57.92\n",
            "64.31\n",
            "69.32\n",
            "71.49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ADAM(epochs, learningRate, batchSize, betha1, betha2, epsioln):\n",
        "  layers = 3\n",
        "  nodes  = [128, 64, 10]\n",
        "  inputSize  = 28*28\n",
        "  outputSize = 10\n",
        "  iteration = 0\n",
        "\n",
        "  W, B, preActivation, activation  = Initialise(layers, nodes, inputSize, outputSize)\n",
        "  v_W, v_B = InitialiseEmptyWeightsAndBiases(layers, nodes, inputSize)\n",
        "  m_W, m_B = InitialiseEmptyWeightsAndBiases(layers, nodes, inputSize)\n",
        "  mhat_W, mhat_B = InitialiseEmptyWeightsAndBiases(layers, nodes, inputSize)\n",
        "  vhat_W, vhat_B = InitialiseEmptyWeightsAndBiases(layers, nodes, inputSize)\n",
        "  # inititialising u to be zero\n",
        "  t = 1\n",
        "\n",
        "  while(iteration < epochs):\n",
        "    # this is used to compute the gradients\n",
        "    i = 0\n",
        "    while i < X_train.shape[1]:\n",
        "      y = FeedForward(X_train[:, i:i+batchSize], W, B, preActivation, activation)\n",
        "      dw, db = BackWardPropogation(X_train[:, i:i+batchSize], Y_train[i:i+batchSize], W, preActivation, activation, y)\n",
        "\n",
        "      # updating the momentum\n",
        "      for k in range(1, layers+1):\n",
        "        m_W[k] = betha1*m_W[k] + (1 - betha1)*dw[k]\n",
        "        m_B[k] = betha1*m_B[k] + (1 - betha1)*db[k]\n",
        "\n",
        "        # finding m hat of W and B\n",
        "        mhat_W[k] = m_W[k]/(1 - betha1 ** t)\n",
        "        mhat_B[k] = m_B[k]/(1 - betha1 ** t)\n",
        "\n",
        "      # update the v values with the gradient\n",
        "      for k in range(1, layers+1):\n",
        "        v_W[k] = v_W[k]*betha2 + (1 - betha2) * (dw[k] ** 2)\n",
        "        v_B[k] = v_B[k]*betha2 + (1 - betha2) * (db[k] ** 2)\n",
        "\n",
        "        # finding v hat of W and B\n",
        "        vhat_W[k] = v_W[k]/(1 - betha2 ** t)\n",
        "        vhat_B[k] = v_B[k]/(1 - betha2 ** t)\n",
        "\n",
        "      # we will update the weights now with the momentum\n",
        "      for k in range(1, layers+1):\n",
        "        l2_norm_w = np.linalg.norm(vhat_W[k])\n",
        "        l2_norm_b = np.linalg.norm(vhat_B[k])\n",
        "        W[k] = W[k] - (learningRate/np.sqrt(l2_norm_w) + epsioln)*mhat_W[k]\n",
        "        B[k] = B[k] - (learningRate/np.sqrt(l2_norm_b) + epsioln)*mhat_B[k]\n",
        "\n",
        "      t += 1\n",
        "      i += batchSize\n",
        "\n",
        "    print(FindAccuracy(W, B, preActivation, activation))\n",
        "    iteration += 1"
      ],
      "metadata": {
        "id": "MLSFG50q87mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ADAM(10, 0.0001, 64, 0.9, 0.999, 0.005)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJxQca1vKoEn",
        "outputId": "5dfe15f4-a2a4-4681-93c8-5ee1c3f8f572"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "72.43\n",
            "78.69\n",
            "81.75\n",
            "83.07\n",
            "83.94\n",
            "84.55\n",
            "85.06\n",
            "85.56\n",
            "85.9\n",
            "86.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def NADAM(epochs, learningRate, batchSize, betha1, betha2, epsioln):\n",
        "  layers = 3\n",
        "  nodes  = [128, 64, 10]\n",
        "  inputSize  = 28*28\n",
        "  outputSize = 10\n",
        "  iteration = 0\n",
        "\n",
        "  W, B, preActivation, activation  = Initialise(layers, nodes, inputSize, outputSize)\n",
        "  v_W, v_B = InitialiseEmptyWeightsAndBiases(layers, nodes, inputSize)\n",
        "  m_W, m_B = InitialiseEmptyWeightsAndBiases(layers, nodes, inputSize)\n",
        "  mhat_W, mhat_B = InitialiseEmptyWeightsAndBiases(layers, nodes, inputSize)\n",
        "  vhat_W, vhat_B = InitialiseEmptyWeightsAndBiases(layers, nodes, inputSize)\n",
        "  # inititialising u to be zero\n",
        "  t = 1\n",
        "\n",
        "  while(iteration < epochs):\n",
        "    # this is used to compute the gradients\n",
        "    i = 0\n",
        "    while i < X_train.shape[1]:\n",
        "      y = FeedForward(X_train[:, i:i+batchSize], W, B, preActivation, activation)\n",
        "      dw, db = BackWardPropogation(X_train[:, i:i+batchSize], Y_train[i:i+batchSize], W, preActivation, activation, y)\n",
        "\n",
        "      # updating the momentum\n",
        "      for k in range(1, layers+1):\n",
        "        m_W[k] = betha1*m_W[k] + (1 - betha1)*dw[k]\n",
        "        m_B[k] = betha1*m_B[k] + (1 - betha1)*db[k]\n",
        "\n",
        "        # finding m hat of W and B\n",
        "        mhat_W[k] = m_W[k]/(1 - betha1 ** t)\n",
        "        mhat_B[k] = m_B[k]/(1 - betha1 ** t)\n",
        "\n",
        "      # update the v values with the gradient\n",
        "      for k in range(1, layers+1):\n",
        "        v_W[k] = v_W[k]*betha2 + (1 - betha2) * (dw[k] ** 2)\n",
        "        v_B[k] = v_B[k]*betha2 + (1 - betha2) * (db[k] ** 2)\n",
        "\n",
        "        # finding v hat of W and B\n",
        "        vhat_W[k] = v_W[k]/(1 - betha2 ** t)\n",
        "        vhat_B[k] = v_B[k]/(1 - betha2 ** t)\n",
        "\n",
        "      # we will update the weights now with the momentum\n",
        "      for k in range(1, layers+1):\n",
        "        l2_norm_w = np.linalg.norm(vhat_W[k])\n",
        "        l2_norm_b = np.linalg.norm(vhat_B[k])\n",
        "        W[k] = W[k] - (learningRate/np.sqrt(l2_norm_w) + epsioln)*(mhat_W[k]*betha1 + (1 - betha1)*dw[k]/(1 - betha1 ** t))\n",
        "        B[k] = B[k] - (learningRate/np.sqrt(l2_norm_b) + epsioln)*(mhat_B[k]*betha1 + (1 - betha1)*db[k]/(1 - betha1 ** t))\n",
        "\n",
        "      t += 1\n",
        "      i += batchSize\n",
        "\n",
        "    print(FindAccuracy(W, B, preActivation, activation))\n",
        "    iteration += 1"
      ],
      "metadata": {
        "id": "9LcKJBFnKoAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NADAM(10, 0.001, 16, 0.9, 0.999, 0.005)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZ9moy55Kn0o",
        "outputId": "7bef2b0d-c659-4419-d369-8082e81a055a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "72.97\n",
            "81.62\n",
            "84.1\n",
            "85.02\n",
            "85.65\n",
            "86.11\n",
            "86.61\n",
            "86.88\n",
            "87.22\n",
            "87.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NADAM(10, 0.0001, 64, 0.9, 0.999, 0.005)"
      ],
      "metadata": {
        "id": "jf1REkuolTvY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42e7538c-40aa-4636-a02c-6418017b5214"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "62.5\n",
            "76.76\n",
            "81.6\n",
            "83.73\n",
            "84.57\n",
            "85.49\n",
            "86.09\n",
            "86.43\n",
            "86.9\n",
            "87.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5E5RDRdueH6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nKt2zCVieH3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dPVrV0TweH0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Writing in the class wise fashion"
      ],
      "metadata": {
        "id": "gb4e7iAVeIun"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_bguDrYZeHv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aUuuy3MTeHrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(x):\n",
        "  return np.maximum(0, x)\n",
        "\n",
        "def sigmoid(x):\n",
        "  clip_x = np.clip(x, -500, 500)  # Clipping x to avoid overflow\n",
        "  return 1 / (1 + np.exp(-clip_x))\n",
        "\n",
        "def _tanh(x):\n",
        "  clip_x = np.clip(x, -500, 500)  # Clipping x for uniformity\n",
        "  return np.tanh(clip_x)"
      ],
      "metadata": {
        "id": "9ppJqUZT0Ms8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## writing the code in the classwise fashion\n",
        "class NeuralNetwork:\n",
        "  def __init__(self, inputSize, hiddenLayers, outputSize, sizeOfHiddenLayers, batchSize, learningRate, initialisationType, optimiser, epochs, activationFunc, weightDecay, isWandb = False, lossFunc = \"cross_entropy\", dataset = \"fashion_mnist\"):\n",
        "    # initialising model parameters\n",
        "    nodes_in_layers = []\n",
        "    for i in range(hiddenLayers):\n",
        "      nodes_in_layers.append(sizeOfHiddenLayers)\n",
        "    nodes_in_layers.append(outputSize)\n",
        "    if dataset == \"fashion_mnist\":\n",
        "      (X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()\n",
        "      X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "    # normalsing and resisizing all the images\n",
        "    X_train = X_train/255.0\n",
        "    X_test  = X_test/255.0\n",
        "    X_val   = X_val/255.0\n",
        "\n",
        "    X_train = X_train.reshape(X_train.shape[0], 784).T\n",
        "    X_test = X_test.reshape(X_test.shape[0], 784).T\n",
        "    X_val = X_val.reshape(X_val.shape[0], 784).T\n",
        "\n",
        "    self.X_train = X_train\n",
        "    self.Y_train = Y_train\n",
        "    self.X_val   = X_val\n",
        "    self.Y_val   = Y_val\n",
        "    self.X_test  = X_test\n",
        "    self.Y_test  = Y_test\n",
        "\n",
        "    self.inputSize = inputSize\n",
        "    self.outputSize= outputSize\n",
        "    self.batchSize = batchSize\n",
        "    self.layers = hiddenLayers + 1\n",
        "    self.nodes  = nodes_in_layers\n",
        "    self.initialisationType = initialisationType\n",
        "    self.betha1 = 0.9\n",
        "    self.betha2 = 0.999\n",
        "    self.betha  = 0.9\n",
        "    self.epsilon= 1e-8\n",
        "    self.Weights= {}\n",
        "    self.Baises = {}\n",
        "    self.optimiser = optimiser\n",
        "    self.epochs = epochs\n",
        "    self.learningRate = learningRate\n",
        "    self.activationFunc = activationFunc\n",
        "    self.isWandb = isWandb\n",
        "    self.weightDecay = weightDecay\n",
        "    self.lossFunc = lossFunc # \"cross_entropy\" or \"MSE\"\n",
        "\n",
        "\n",
        "  def Initialise(self):\n",
        "    # initialising weights and biases as a key value pair\n",
        "    W = {}\n",
        "    B = {}\n",
        "\n",
        "    PreActivation = {}\n",
        "    Activation = {}\n",
        "\n",
        "    # adding input layer\n",
        "    LayerWise = self.nodes\n",
        "    LayerWise.insert(0, self.inputSize)\n",
        "\n",
        "    # initialisation of weights and baises\n",
        "    for i in range(self.layers):\n",
        "      if self.initialisationType == \"random\":\n",
        "        W[i+1] = 0.01*np.random.randn(LayerWise[i+1], LayerWise[i])\n",
        "        B[i+1] = 0.01*np.random.randn(LayerWise[i+1], 1)\n",
        "      if self.initialisationType == \"Xavier\":\n",
        "        W[i+1] = np.random.randn(LayerWise[i+1], LayerWise[i]) * np.sqrt(2. / (LayerWise[i] + LayerWise[i+1]))\n",
        "        B[i+1] = np.zeros((LayerWise[i+1], 1))\n",
        "\n",
        "      # preactivation and activation will have same size\n",
        "      PreActivation[i+1] = np.zeros((LayerWise[i+1], 1))\n",
        "      Activation[i+1]    = np.zeros((LayerWise[i+1], 1))\n",
        "\n",
        "    del LayerWise[0]\n",
        "\n",
        "    self.Weights = W\n",
        "    self.Baises  = B\n",
        "    self.PreActivation = PreActivation\n",
        "    self.Activation = Activation\n",
        "\n",
        "    return W,B,PreActivation,Activation\n",
        "\n",
        "  def InitialiseEmptyWeightsAndBiases(self):\n",
        "    W = {}\n",
        "    B = {}\n",
        "    LayerWise = self.nodes\n",
        "\n",
        "    LayerWise.insert(0, self.inputSize)\n",
        "    for i in range(self.layers):\n",
        "      W[i+1] = np.zeros((LayerWise[i+1], LayerWise[i]))\n",
        "      B[i+1] = np.zeros((LayerWise[i+1], 1))\n",
        "    del LayerWise[0]\n",
        "\n",
        "    return W,B\n",
        "\n",
        "  def FeedForward(self, x, W, B, preActivation, activation):\n",
        "    # no of layers\n",
        "    n = len(W)\n",
        "    y = x\n",
        "    for i in range(1, n+1):\n",
        "      preActivation[i] = np.dot(W[i], y) + B[i]\n",
        "      if self.activationFunc == \"sigmoid\":\n",
        "        activation[i] = sigmoid(preActivation[i])\n",
        "      elif self.activationFunc == \"tanh\":\n",
        "        activation[i] = _tanh(preActivation[i])\n",
        "      elif self.activationFunc == \"relu\":\n",
        "        activation[i] = relu(preActivation[i])\n",
        "\n",
        "      y = activation[i]\n",
        "\n",
        "    # last layer we don't need activation\n",
        "    y = preActivation[n]\n",
        "    # doing softmax doing the each column wise\n",
        "    exp_y = np.exp(y - np.max(y, axis=0, keepdims=True))  # Improve numerical stability\n",
        "    y = exp_y / np.sum(exp_y, axis=0, keepdims=True)\n",
        "    return y\n",
        "\n",
        "  def BackWardPropogation(self, X, y_corr, W, preActivation, activation, y_hat):\n",
        "    # y_hat is the prediction and y_corr is the correct class\n",
        "    dw = {}\n",
        "    db = {}\n",
        "\n",
        "    # these many points are there in the batch\n",
        "    batchSize = y_corr.shape[0]\n",
        "    da_l = y_hat\n",
        "\n",
        "    ind = 0\n",
        "    while ind < batchSize:\n",
        "      da_l[y_corr[ind]][ind] -= 1\n",
        "      ind += 1\n",
        "    da = da_l\n",
        "\n",
        "    activation[0] = X\n",
        "    layer = len(W)\n",
        "    dh = da #used for finding next layer\n",
        "    while layer >= 1:\n",
        "      dw[layer] = np.dot(da, activation[layer-1].T)\n",
        "      db[layer] = da.sum(axis=1, keepdims=True)\n",
        "      if layer > 1:\n",
        "        dh = np.dot(W[layer].T, da)\n",
        "        if self.activationFunc == \"sigmoid\":\n",
        "          dg = activation[layer-1] * (1 - activation[layer-1])\n",
        "        elif self.activationFunc == \"tanh\":\n",
        "          dg = (1 + activation[layer-1]) * (1 - activation[layer-1])\n",
        "        elif self.activationFunc == \"relu\":\n",
        "          dg = np.where(preActivation[layer-1] > 0, 1, 0)\n",
        "\n",
        "        # dg = activation[layer-1] * (1 - activation[layer-1])\n",
        "        da = dh * dg\n",
        "        # hedamant product\n",
        "      layer -= 1\n",
        "\n",
        "    for i in range(1, self.layers+1):\n",
        "      dw[i] = dw[i] + self.weightDecay*W[i]\n",
        "\n",
        "    return dw, db\n",
        "\n",
        "  def FindAccuracyAndLoss(self, W, B, data, labels):\n",
        "    n = data.shape[1]\n",
        "    correct = 0\n",
        "\n",
        "    #running the data on the weights and baises\n",
        "    y = data\n",
        "    for i in range(1, self.layers+1):\n",
        "      preActivation = np.dot(W[i], y) + B[i]\n",
        "\n",
        "      if self.activationFunc == \"sigmoid\":\n",
        "        activation = sigmoid(preActivation)\n",
        "      elif self.activationFunc == \"tanh\":\n",
        "        activation = _tanh(preActivation)\n",
        "      elif self.activationFunc == \"relu\":\n",
        "        activation = relu(preActivation)\n",
        "      y = activation\n",
        "\n",
        "    # last layer we don't need activation\n",
        "    y = preActivation\n",
        "    # doing softmax doing the each column wise\n",
        "    exp_y = np.exp(y - np.max(y, axis=0, keepdims=True))  # Improve numerical stability\n",
        "    y = exp_y / np.sum(exp_y, axis=0, keepdims=True)\n",
        "    loss = 0\n",
        "\n",
        "    for i in range(n):\n",
        "      y_pred = np.argmax(y[:,i])\n",
        "      if labels[i] == y_pred:\n",
        "        correct += 1\n",
        "\n",
        "      loss += -1*np.log(y[:,i][labels[i]] + 1e-9)\n",
        "\n",
        "    return (correct*100/ n), (loss/n)\n",
        "\n",
        "  def SGD(self):\n",
        "    W, B, preActivation, activation  = self.Initialise()\n",
        "    iteration = 0\n",
        "    layers = self.layers\n",
        "    empty_W, empty_B = self.InitialiseEmptyWeightsAndBiases()\n",
        "\n",
        "    while(iteration < self.epochs):\n",
        "      i = 0\n",
        "      while i < self.X_train.shape[1]:\n",
        "        y = self.FeedForward(self.X_train[:, i:i+self.batchSize], W, B, preActivation, activation)\n",
        "        # these are the partial derivates for one point\n",
        "        dw, db = self.BackWardPropogation(self.X_train[:, i:i+self.batchSize], self.Y_train[i:i+self.batchSize], W, preActivation, activation, y)\n",
        "\n",
        "        # we will update the weights now\n",
        "        for k in range(1, layers+1):\n",
        "            W[k] = W[k] - self.learningRate*dw[k]\n",
        "            B[k] = B[k] - self.learningRate*db[k]\n",
        "\n",
        "        i += self.batchSize\n",
        "      acuu, loss = self.FindAccuracyAndLoss(W, B, self.X_train, self.Y_train)\n",
        "      v_acc, v_loss = self.FindAccuracyAndLoss(W, B, self.X_val, self.Y_val)\n",
        "      if self.isWandb == True:\n",
        "        wandb.log({'accuracy': acuu})\n",
        "        wandb.log({'loss': loss})\n",
        "        wandb.log({'v_accuracy': v_acc})\n",
        "        wandb.log({'v_loss': v_loss})\n",
        "      print(acuu, loss, v_acc, v_loss)\n",
        "      iteration += 1\n",
        "\n",
        "    self.Weights = W\n",
        "    self.Baises  = B\n",
        "\n",
        "  def MomentBasedGradientDecent(self):\n",
        "    W, B, preActivation, activation  = self.Initialise()\n",
        "    iteration = 0\n",
        "    u_W, u_B = self.InitialiseEmptyWeightsAndBiases()\n",
        "    # inititialising u to be zero\n",
        "\n",
        "    while(iteration < self.epochs):\n",
        "      i = 0\n",
        "      while i < self.X_train.shape[1]:\n",
        "        # batch wise forward and backward passes\n",
        "        y = self.FeedForward(self.X_train[:, i:i+self.batchSize], W, B, preActivation, activation)\n",
        "        dw, db = self.BackWardPropogation(self.X_train[:, i:i+self.batchSize], self.Y_train[i:i+self.batchSize], W, preActivation, activation, y)\n",
        "\n",
        "        # update the momentum with the gradient\n",
        "        for k in range(1, self.layers+1):\n",
        "          u_W[k] = u_W[k]*self.betha + dw[k]\n",
        "          u_B[k] = u_B[k]*self.betha + db[k]\n",
        "\n",
        "        # we will update the weights now with the momentum\n",
        "        for k in range(1, self.layers+1):\n",
        "            W[k] = W[k] - self.learningRate*u_W[k]\n",
        "            B[k] = B[k] - self.learningRate*u_B[k]\n",
        "\n",
        "        # next batch\n",
        "        i += self.batchSize\n",
        "      acuu, loss = self.FindAccuracyAndLoss(W, B, self.X_train, self.Y_train)\n",
        "      v_acc, v_loss = self.FindAccuracyAndLoss(W, B, self.X_val, self.Y_val)\n",
        "      if self.isWandb == True:\n",
        "        wandb.log({'accuracy': acuu})\n",
        "        wandb.log({'loss': loss})\n",
        "        wandb.log({'v_accuracy': v_acc})\n",
        "        wandb.log({'v_loss': v_loss})\n",
        "      print(acuu, loss, v_acc, v_loss)\n",
        "      iteration += 1\n",
        "\n",
        "    self.Weights = W\n",
        "    self.Baises  = B\n",
        "\n",
        "  def NestrovBasedGradientDescent(self):\n",
        "    iteration = 0\n",
        "    W, B, preActivation, activation = self.Initialise()\n",
        "    u_W, u_B = self.InitialiseEmptyWeightsAndBiases()\n",
        "    # initializing u to be zero\n",
        "\n",
        "    while(iteration < self.epochs):\n",
        "      i = 0\n",
        "      while i < self.X_train.shape[1]:\n",
        "\n",
        "        y = self.FeedForward(self.X_train[:, i:i+self.batchSize], W, B, preActivation, activation)\n",
        "        dw, db = self.BackWardPropogation(self.X_train[:, i:i+self.batchSize], self.Y_train[i:i+self.batchSize], W, preActivation, activation, y)\n",
        "\n",
        "        for k in range(1, self.layers+1):\n",
        "            u_W[k] = u_W[k]*self.betha + dw[k]\n",
        "            u_B[k] = u_B[k]*self.betha + db[k]\n",
        "\n",
        "        for k in range(1, self.layers+1):\n",
        "            W[k] = W[k] - self.learningRate*(self.betha* u_W[k]+ dw[k])\n",
        "            B[k] = B[k] - self.learningRate*(self.betha* u_B[k]+ db[k])\n",
        "\n",
        "        i += self.batchSize\n",
        "      acuu, loss = self.FindAccuracyAndLoss(W, B, self.X_train, self.Y_train)\n",
        "      v_acc, v_loss = self.FindAccuracyAndLoss(W, B, self.X_val, self.Y_val)\n",
        "      if self.isWandb == True:\n",
        "        wandb.log({'accuracy': acuu})\n",
        "        wandb.log({'loss': loss})\n",
        "        wandb.log({'v_accuracy': v_acc})\n",
        "        wandb.log({'v_loss': v_loss})\n",
        "      print(acuu, loss, v_acc, v_loss)\n",
        "      iteration += 1\n",
        "\n",
        "    self.Weights = W\n",
        "    self.Baises  = B\n",
        "\n",
        "  def RMSPROP(self):\n",
        "    iteration = 0\n",
        "    epochs = self.epochs\n",
        "    layers = self.layers\n",
        "    batchSize = self.batchSize\n",
        "    betha = self.betha\n",
        "    W, B, preActivation, activation  = self.Initialise()\n",
        "    v_W, v_B = self.InitialiseEmptyWeightsAndBiases()\n",
        "    # inititialising u to be zero\n",
        "\n",
        "    while(iteration < epochs):\n",
        "      i = 0\n",
        "      while i < self.X_train.shape[1]:\n",
        "        y = self.FeedForward(self.X_train[:, i:i+batchSize], W, B, preActivation, activation)\n",
        "        dw, db = self.BackWardPropogation(self.X_train[:, i:i+batchSize], self.Y_train[i:i+batchSize], W, preActivation, activation, y)\n",
        "\n",
        "        # update the v values with the gradient\n",
        "        for k in range(1, layers+1):\n",
        "          v_W[k] = v_W[k]*betha + (1 - betha) * (dw[k] ** 2)\n",
        "          v_B[k] = v_B[k]*betha + (1 - betha) * (db[k] ** 2)\n",
        "\n",
        "        # we will update the weights now with the momentum\n",
        "        for k in range(1, layers+1):\n",
        "          W[k] = W[k] - (self.learningRate/np.sqrt(v_W[k] + self.epsilon))*dw[k]\n",
        "          B[k] = B[k] - (self.learningRate/np.sqrt(v_B[k] + self.epsilon))*db[k]\n",
        "\n",
        "        i += batchSize\n",
        "      acuu, loss = self.FindAccuracyAndLoss(W, B, self.X_train, self.Y_train)\n",
        "      v_acc, v_loss = self.FindAccuracyAndLoss(W, B, self.X_val, self.Y_val)\n",
        "      if self.isWandb == True:\n",
        "        wandb.log({'accuracy': acuu})\n",
        "        wandb.log({'loss': loss})\n",
        "        wandb.log({'v_accuracy': v_acc})\n",
        "        wandb.log({'v_loss': v_loss})\n",
        "      print(acuu, loss, v_acc, v_loss)\n",
        "      iteration += 1\n",
        "\n",
        "    self.Weights = W\n",
        "    self.Baises  = B\n",
        "\n",
        "  def ADAM(self):\n",
        "    iteration = 0\n",
        "    epochs = self.epochs\n",
        "    layers = self.layers\n",
        "    batchSize = self.batchSize\n",
        "\n",
        "    W, B, preActivation, activation  = self.Initialise()\n",
        "    v_W, v_B = self.InitialiseEmptyWeightsAndBiases()\n",
        "    m_W, m_B = self.InitialiseEmptyWeightsAndBiases()\n",
        "    mhat_W, mhat_B = self.InitialiseEmptyWeightsAndBiases()\n",
        "    vhat_W, vhat_B = self.InitialiseEmptyWeightsAndBiases()\n",
        "    # inititialising u to be zero\n",
        "    t = 1\n",
        "\n",
        "    while(iteration < epochs):\n",
        "      # this is used to compute the gradients\n",
        "      i = 0\n",
        "      while i < self.X_train.shape[1]:\n",
        "        y = self.FeedForward(self.X_train[:, i:i+batchSize], W, B, preActivation, activation)\n",
        "        dw, db = self.BackWardPropogation(self.X_train[:, i:i+batchSize], self.Y_train[i:i+batchSize], W, preActivation, activation, y)\n",
        "\n",
        "        # updating the momentum\n",
        "        for k in range(1, layers+1):\n",
        "          m_W[k] = self.betha1*m_W[k] + (1 - self.betha1)*dw[k]\n",
        "          m_B[k] = self.betha1*m_B[k] + (1 - self.betha1)*db[k]\n",
        "\n",
        "          # finding m hat of W and B\n",
        "          mhat_W[k] = m_W[k]/(1 - self.betha1 ** t)\n",
        "          mhat_B[k] = m_B[k]/(1 - self.betha1 ** t)\n",
        "\n",
        "        # update the v values with the gradient\n",
        "        for k in range(1, layers+1):\n",
        "          v_W[k] = v_W[k]*self.betha2 + (1 - self.betha2) * (dw[k] ** 2)\n",
        "          v_B[k] = v_B[k]*self.betha2 + (1 - self.betha2) * (db[k] ** 2)\n",
        "\n",
        "          # finding v hat of W and B\n",
        "          vhat_W[k] = v_W[k]/(1 - self.betha2 ** t)\n",
        "          vhat_B[k] = v_B[k]/(1 - self.betha2 ** t)\n",
        "\n",
        "        # we will update the weights now with the momentum\n",
        "        for k in range(1, layers+1):\n",
        "          l2_norm_w = np.linalg.norm(vhat_W[k])\n",
        "          l2_norm_b = np.linalg.norm(vhat_B[k])\n",
        "          W[k] = W[k] - (self.learningRate/np.sqrt(l2_norm_w) + self.epsilon)*mhat_W[k]\n",
        "          B[k] = B[k] - (self.learningRate/np.sqrt(l2_norm_b) + self.epsilon)*mhat_B[k]\n",
        "\n",
        "        t += 1\n",
        "        i += self.batchSize\n",
        "\n",
        "      acuu, loss = self.FindAccuracyAndLoss(W, B, self.X_train, self.Y_train)\n",
        "      v_acc, v_loss = self.FindAccuracyAndLoss(W, B, self.X_val, self.Y_val)\n",
        "      if self.isWandb == True:\n",
        "        wandb.log({'accuracy': acuu})\n",
        "        wandb.log({'loss': loss})\n",
        "        wandb.log({'v_accuracy': v_acc})\n",
        "        wandb.log({'v_loss': v_loss})\n",
        "      print(acuu, loss, v_acc, v_loss)\n",
        "      iteration += 1\n",
        "\n",
        "    self.Weights = W\n",
        "    self.Baises  = B\n",
        "\n",
        "  def NADAM(self):\n",
        "    iteration = 0\n",
        "    epochs = self.epochs\n",
        "    layers = self.layers\n",
        "    W, B, preActivation, activation  = self.Initialise()\n",
        "    v_W, v_B = self.InitialiseEmptyWeightsAndBiases()\n",
        "    m_W, m_B = self.InitialiseEmptyWeightsAndBiases()\n",
        "    mhat_W, mhat_B = self.InitialiseEmptyWeightsAndBiases()\n",
        "    vhat_W, vhat_B = self.InitialiseEmptyWeightsAndBiases()\n",
        "    # inititialising u to be zero\n",
        "    t = 1\n",
        "\n",
        "    while(iteration < self.epochs):\n",
        "      # this is used to compute the gradients\n",
        "      i = 0\n",
        "      while i < self.X_train.shape[1]:\n",
        "        y = self.FeedForward(self.X_train[:, i:i+self.batchSize], W, B, preActivation, activation)\n",
        "        dw, db = self.BackWardPropogation(self.X_train[:, i:i+self.batchSize], self.Y_train[i:i+self.batchSize], W, preActivation, activation, y)\n",
        "\n",
        "        # updating the momentum\n",
        "        for k in range(1, layers+1):\n",
        "          m_W[k] = self.betha1*m_W[k] + (1 - self.betha1)*dw[k]\n",
        "          m_B[k] = self.betha1*m_B[k] + (1 - self.betha1)*db[k]\n",
        "\n",
        "          # finding m hat of W and B\n",
        "          mhat_W[k] = m_W[k]/(1 - self.betha1 ** t)\n",
        "          mhat_B[k] = m_B[k]/(1 - self.betha1 ** t)\n",
        "\n",
        "        # update the v values with the gradient\n",
        "        for k in range(1, layers+1):\n",
        "          v_W[k] = v_W[k]*self.betha2 + (1 - self.betha2) * (dw[k] ** 2)\n",
        "          v_B[k] = v_B[k]*self.betha2 + (1 - self.betha2) * (db[k] ** 2)\n",
        "\n",
        "          # finding v hat of W and B\n",
        "          vhat_W[k] = v_W[k]/(1 - self.betha2 ** t)\n",
        "          vhat_B[k] = v_B[k]/(1 - self.betha2 ** t)\n",
        "\n",
        "        # we will update the weights now with the momentum\n",
        "        for k in range(1, layers+1):\n",
        "          l2_norm_w = np.linalg.norm(vhat_W[k])\n",
        "          l2_norm_b = np.linalg.norm(vhat_B[k])\n",
        "          W[k] = W[k] - (self.learningRate/np.sqrt(l2_norm_w) + self.epsilon)*(mhat_W[k]*self.betha1 + (1 - self.betha1)*dw[k]/(1 - self.betha1 ** t))\n",
        "          B[k] = B[k] - (self.learningRate/np.sqrt(l2_norm_b) + self.epsilon)*(mhat_B[k]*self.betha1 + (1 - self.betha1)*db[k]/(1 - self.betha1 ** t))\n",
        "\n",
        "        t += 1\n",
        "        i += self.batchSize\n",
        "\n",
        "      acuu, loss = self.FindAccuracyAndLoss(W, B, self.X_train, self.Y_train)\n",
        "      v_acc, v_loss = self.FindAccuracyAndLoss(W, B, self.X_val, self.Y_val)\n",
        "      if self.isWandb == True:\n",
        "        wandb.log({'accuracy': acuu})\n",
        "        wandb.log({'loss': loss})\n",
        "        wandb.log({'v_accuracy': v_acc})\n",
        "        wandb.log({'v_loss': v_loss})\n",
        "      print(acuu, loss, v_acc, v_loss)\n",
        "      iteration += 1\n",
        "\n",
        "    self.Weights = W\n",
        "    self.Baises  = B\n",
        "\n",
        "  def fit(self):\n",
        "    if self.optimiser == \"sgd\":\n",
        "      self.SGD()\n",
        "    if self.optimiser == \"momentum\":\n",
        "      self.MomentBasedGradientDecent()\n",
        "    if self.optimiser == \"nestrov\":\n",
        "      self.NestrovBasedGradientDescent()\n",
        "    if self.optimiser == \"rmsprop\":\n",
        "      self.RMSPROP()\n",
        "    if self.optimiser == \"adam\":\n",
        "      self.ADAM()\n",
        "    if self.optimiser == \"nadam\":\n",
        "      self.NADAM()\n"
      ],
      "metadata": {
        "id": "-sX2Tw9RXBPt"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetwork(inputSize = 784, hiddenLayers = 4, outputSize = 10, sizeOfHiddenLayers = 64, batchSize = 32, learningRate = 0.001, initialisationType = \"Xavier\", optimiser = \"nadam\", activationFunc=\"relu\",weightDecay = 0.0005, epochs = 4)\n",
        "model.fit()"
      ],
      "metadata": {
        "id": "4u1U61YTZxb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    wandb.init(project=\"Assignment 1\")\n",
        "    config = wandb.config\n",
        "    run_name = f\"{config.optimiser}_{config.activation}_{config.hidden_layers}_{config.hidden_layer_size}_{config.batch_size}\"\n",
        "\n",
        "    # Set the run name\n",
        "    wandb.run.name = run_name\n",
        "    wandb.run.save()\n",
        "\n",
        "    # Define and train the model as before\n",
        "    model = NeuralNetwork(inputSize = 784, hiddenLayers = config.hidden_layers, outputSize = 10, sizeOfHiddenLayers = config.hidden_layer_size, batchSize = config.batch_size, learningRate = config.learning_rate, initialisationType = config.weights_initialisation, optimiser = config.optimiser, activationFunc=config.activation, epochs = config.epochs,weightDecay = config.weight_decay, isWandb = True)\n",
        "    model.fit()\n",
        "    wandb.finish()\n",
        "\n",
        "sweep_config = {\n",
        "    'method': 'bayes',\n",
        "    'name' : 'sweep cross entropy',\n",
        "    'metric': {\n",
        "      'name': 'v_accuracy',\n",
        "      'goal': 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'epochs': {\n",
        "            'values': [5,10]\n",
        "        },\n",
        "        'hidden_layers': {\n",
        "          'values': [3, 4, 5]\n",
        "        },\n",
        "        'hidden_layer_size':{\n",
        "            'values':[32,64,128]\n",
        "        },\n",
        "        'weight_decay': {\n",
        "            'values':[0, 0.0005, 0.5]\n",
        "        },\n",
        "        'batch_size': {\n",
        "            'values': [16, 32, 64]\n",
        "        },\n",
        "        'activation': {\n",
        "            'values': ['sigmoid','relu','tanh']\n",
        "        },\n",
        "        'optimiser': {\n",
        "            'values': ['sgd', 'momentum', 'nestrov', 'rmsprop', 'adam', 'nadam']\n",
        "        },\n",
        "        'weights_initialisation': {\n",
        "            'values': ['random', 'Xavier']\n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'values':[1e-2,1e-3,1e-4]\n",
        "        },\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "sweep_id = wandb.sweep(sweep=sweep_config,project='Assignment 1')\n",
        "wandb.agent(sweep_id , function = main , count = 150)\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "Keiyle7_fYg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## todo tasks\n",
        "\n",
        "#  -- Add different activation functions and derivaties still linear is left\n",
        "#  -- L2 regularisation by adding epsilon * alpha * w at each updation so send this at the dw find\n",
        "#"
      ],
      "metadata": {
        "id": "69f2ObJ-fYXq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}